Overview

This project is a simple artificial neural network implemented from scratch in Java without using any machine learning libraries. The goal of the project is to understand the internal workings of neural networks, including forward propagation, backpropagation, weight updates, and matrix operations.

Motivation

The project was built to strengthen understanding of how neural networks function at a low level, rather than relying on high-level frameworks. It focuses on learning the math, data flow, and architecture behind neural networks.

Features

Fully connected feedforward neural network

Custom implementation of neurons, layers, weights, and biases

Forward propagation and backpropagation

Gradient-based weight and bias updates

Configurable network architecture (input, hidden, output layers)

No external ML libraries used

Technologies Used

Java

Basic linear algebra and matrix operations

What I Learned

Neural network architecture and data flow

Forward and backward propagation mechanics

Gradient descent and loss minimization

Handling dimensionality and matrix operations

Implementing learning algorithms from first principles

Project Structure

Main.java Entire project 

How It Works

Input data is passed through the network using forward propagation.

The output is compared against the expected result to calculate error.

Backpropagation computes gradients for weights and biases.

Parameters are updated using gradient descent.

The process repeats over multiple training iterations.

How to Run

Clone the repository

Compile the Java files

Run the main class to train and test the network

Make sure to reference the MNIST dataset to your local machine be yourself. 

Author

Avash
